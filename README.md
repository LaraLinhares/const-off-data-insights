# ğŸŒªï¸ Projeto de Big-Data: AnÃ¡lise de Constrained-Off EÃ³lico

## ğŸ“‹ DescriÃ§Ã£o do Projeto

Este projeto implementa uma soluÃ§Ã£o completa de big-data para anÃ¡lise de constrained-off (restriÃ§Ãµes de geraÃ§Ã£o) em usinas eÃ³licas brasileiras, utilizando dados consolidados do ONS (Operador Nacional do Sistema ElÃ©trico).

### ğŸ¯ Objetivos

1. **Processamento** dos dados consolidados via lotes
2. **AgregaÃ§Ãµes temporais e espaciais** com conversÃ£o para formato Parquet
3. **DetecÃ§Ã£o de padrÃµes e anomalias** nos eventos de constrained-off
4. **VisualizaÃ§Ã£o interativa** das anÃ¡lises
5. **AvaliaÃ§Ã£o da arquitetura** proposta

## ğŸ“Š Estrutura dos Dados

### Dados de Entrada (Consolidados)
- **Arquivo Principal**: `restricoes_coff_eolicas_consolidado.csv`
  - GeraÃ§Ã£o, disponibilidade, restriÃ§Ãµes por usina
  - Dados consolidados de todos os meses
  - Granularidade: 30 minutos

- **Arquivo de Detalhamento**: `restricoes_coff_eolicas_detalhamento_consolidado.csv`
  - Dados de vento, geraÃ§Ã£o estimada vs verificada
  - InformaÃ§Ãµes por unidade geradora

### PerÃ­odo de AnÃ¡lise
- **InÃ­cio**: Outubro/2021
- **Fim**: Abril/2025
- **Total**: 42 meses de dados
- **Volume**: Dados consolidados de constrained-off eÃ³lico

## ğŸ—ï¸ Arquitetura da SoluÃ§Ã£o

```
Consolidated CSV Files â†’ Parquet Processing â†’ Temporal/Spatial Aggregations â†’ Anomaly Detection â†’ Visualization
```

### Componentes Principais

1. **Pipeline de Processamento** (`big_data_pipeline.py`)
   - Carregamento de dados consolidados
   - Limpeza e normalizaÃ§Ã£o
   - ConversÃ£o para Parquet

2. **DetecÃ§Ã£o de Anomalias** (`anomaly_detection_analysis.py`)
   - AnÃ¡lise de constrained-off extremo
   - VariaÃ§Ãµes bruscas na geraÃ§Ã£o
   - PadrÃµes temporais anÃ´malos
   - Clusters espaciais

3. **Dashboard Interativo** (`visualization_dashboard.py`)
   - VisualizaÃ§Ãµes em tempo real
   - Filtros dinÃ¢micos
   - RelatÃ³rios automÃ¡ticos

## ğŸ” Tipos de Anomalias Detectadas

### 1. Constrained-Off Extremo
- **CritÃ©rio**: >70% de restriÃ§Ã£o mÃ©dia
- **DetecÃ§Ã£o**: Usinas com perda significativa de geraÃ§Ã£o
- **Impacto**: Perda de energia renovÃ¡vel

### 2. VariaÃ§Ãµes Bruscas na GeraÃ§Ã£o
- **CritÃ©rio**: Z-score >3 na geraÃ§Ã£o
- **DetecÃ§Ã£o**: Instabilidade na produÃ§Ã£o
- **Impacto**: Instabilidade na rede

### 3. PadrÃµes Temporais AnÃ´malos
- **CritÃ©rio**: HorÃ¡rios com >1.5x a mÃ©dia de constrained-off
- **DetecÃ§Ã£o**: PadrÃµes recorrentes de restriÃ§Ã£o
- **Impacto**: IneficiÃªncia operacional

### 4. Clusters Espaciais
- **CritÃ©rio**: Estados com >3 usinas e >30% constrained-off
- **DetecÃ§Ã£o**: Problemas regionais de transmissÃ£o
- **Impacto**: LimitaÃ§Ãµes de infraestrutura

## ğŸš€ Como Executar

### 1. InstalaÃ§Ã£o das DependÃªncias

```bash
# Ativar ambiente virtual
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate     # Windows

# Instalar dependÃªncias
pip install -r requirements.txt
```

### 2. Verificar Dados Consolidados

Certifique-se de que os arquivos consolidados existem:
- `restricoes_coff_eolicas_consolidado.csv`
- `restricoes_coff_eolicas_detalhamento_consolidado.csv`

Se nÃ£o existirem, execute primeiro:
```bash
python transform.py
```

### 3. ExecuÃ§Ã£o do Pipeline Completo

```bash
# Executar pipeline completo
python run_pipeline.py

# Ou executar passos individuais
python run_pipeline.py --steps 2 3  # Apenas pipeline e anomalias
```

### 4. ExecuÃ§Ã£o Individual dos Componentes

```bash
# Apenas pipeline de big-data
python big_data_pipeline.py

# Apenas anÃ¡lise de anomalias
python anomaly_detection_analysis.py

# Apenas dashboard
streamlit run visualization_dashboard.py
```

## ğŸ“ Estrutura de Arquivos

```
data-ons/
â”œâ”€â”€ restricoes_coff_eolicas_consolidado.csv           # Dados principais consolidados
â”œâ”€â”€ restricoes_coff_eolicas_detalhamento_consolidado.csv # Dados de detalhamento
â”œâ”€â”€ processed_data/                                   # Dados processados
â”‚   â”œâ”€â”€ main_batch_*.parquet                         # Dados em lotes
â”‚   â”œâ”€â”€ temporal_summary.parquet                     # AgregaÃ§Ãµes temporais
â”‚   â”œâ”€â”€ state_summary.parquet                        # AgregaÃ§Ãµes por estado
â”‚   â”œâ”€â”€ top_usinas_constrained.parquet               # Top usinas
â”‚   â””â”€â”€ *_analysis.parquet                           # AnÃ¡lises de anomalias
â”œâ”€â”€ big_data_pipeline.py                             # Pipeline principal
â”œâ”€â”€ anomaly_detection_analysis.py                    # AnÃ¡lise de anomalias
â”œâ”€â”€ visualization_dashboard.py                       # Dashboard Streamlit
â”œâ”€â”€ run_pipeline.py                                  # Script principal
â”œâ”€â”€ transform.py                                     # ConsolidaÃ§Ã£o bÃ¡sica
â”œâ”€â”€ requirements.txt                                 # DependÃªncias
â””â”€â”€ README.md                                        # Este arquivo
```

## ğŸ“ˆ Queries SQL para DetecÃ§Ã£o de Anomalias

### 1. Constrained-Off Extremo
```sql
SELECT 
    nom_usina, nom_estado, ano, mes,
    AVG(percentual_constrained) as percentual_medio,
    MAX(percentual_constrained) as percentual_max,
    COUNT(*) as registros
FROM wind_data
WHERE percentual_constrained > 50
GROUP BY nom_usina, nom_estado, ano, mes
HAVING AVG(percentual_constrained) > 70
ORDER BY percentual_medio DESC
```

### 2. VariaÃ§Ãµes Bruscas na GeraÃ§Ã£o
```sql
WITH geracao_stats AS (
    SELECT 
        nom_usina, nom_estado, ano, mes,
        AVG(val_geracao) as geracao_media,
        STDDEV(val_geracao) as geracao_std
    FROM wind_data
    GROUP BY nom_usina, nom_estado, ano, mes
)
SELECT 
    w.nom_usina, w.nom_estado, w.din_instante,
    w.val_geracao, gs.geracao_media, gs.geracao_std,
    ABS(w.val_geracao - gs.geracao_media) / gs.geracao_std as z_score
FROM wind_data w
JOIN geracao_stats gs ON w.nom_usina = gs.nom_usina 
    AND w.ano = gs.ano AND w.mes = gs.mes
WHERE ABS(w.val_geracao - gs.geracao_media) / gs.geracao_std > 3
ORDER BY z_score DESC
```

### 3. PadrÃµes Temporais
```sql
SELECT 
    nom_usina, nom_estado, hora,
    AVG(percentual_constrained) as percentual_medio_hora,
    COUNT(*) as registros_hora
FROM wind_data
GROUP BY nom_usina, nom_estado, hora
HAVING AVG(percentual_constrained) > (
    SELECT AVG(percentual_constrained) * 1.5 
    FROM wind_data 
    WHERE nom_usina = wind_data.nom_usina
)
ORDER BY percentual_medio_hora DESC
```

### 4. Clusters Espaciais
```sql
WITH state_anomalies AS (
    SELECT 
        nom_estado, ano, mes,
        AVG(percentual_constrained) as percentual_estado,
        COUNT(DISTINCT nom_usina) as num_usinas_afetadas
    FROM wind_data
    WHERE percentual_constrained > 30
    GROUP BY nom_estado, ano, mes
)
SELECT 
    nom_estado, ano, mes, percentual_estado, num_usinas_afetadas,
    CASE 
        WHEN percentual_estado > 50 AND num_usinas_afetadas > 5 THEN 'CLUSTER_CRITICO'
        WHEN percentual_estado > 30 AND num_usinas_afetadas > 3 THEN 'CLUSTER_MODERADO'
        ELSE 'ISOLADO'
    END as tipo_cluster
FROM state_anomalies
WHERE percentual_estado > 30
ORDER BY percentual_estado DESC, num_usinas_afetadas DESC
```

## ğŸ¯ Oportunidades de AnÃ¡lise

### 1. PrediÃ§Ã£o de Constrained-Off
- **Dados meteorolÃ³gicos**: IntegraÃ§Ã£o com previsÃµes de vento
- **Machine Learning**: Modelos preditivos baseados em histÃ³rico
- **OtimizaÃ§Ã£o**: ReduÃ§Ã£o de restriÃ§Ãµes via operaÃ§Ã£o

### 2. OtimizaÃ§Ã£o Operacional
- **Despacho inteligente**: RedistribuiÃ§Ã£o de geraÃ§Ã£o
- **Alertas automÃ¡ticos**: DetecÃ§Ã£o em tempo real
- **Planejamento**: ExpansÃ£o da rede de transmissÃ£o

### 3. AnÃ¡lise de Impacto
- **EconÃ´mico**: Custos das restriÃ§Ãµes
- **Ambiental**: Perda de energia renovÃ¡vel
- **TÃ©cnico**: Estabilidade da rede

## ğŸ“Š MÃ©tricas de Sucesso

- **ReduÃ§Ã£o de 20%** no constrained-off total
- **DetecÃ§Ã£o de anomalias** em <5 minutos
- **PrecisÃ£o de prediÃ§Ã£o** >80%
- **ROI positivo** em 12 meses

## ğŸ”§ Tecnologias Utilizadas

- **Python**: Linguagem principal
- **Pandas**: ManipulaÃ§Ã£o de dados
- **NumPy**: ComputaÃ§Ã£o numÃ©rica
- **Plotly**: VisualizaÃ§Ãµes interativas
- **Streamlit**: Dashboard web
- **Parquet**: Formato de armazenamento otimizado
- **SQL**: Queries para detecÃ§Ã£o de anomalias

## ğŸš€ PrÃ³ximos Passos

### Curto Prazo (1-3 meses)
1. Implementar monitoramento em tempo real
2. Criar alertas automÃ¡ticos para anomalias
3. Otimizar despacho das usinas mais afetadas

### MÃ©dio Prazo (3-12 meses)
1. Desenvolver modelos preditivos de constrained-off
2. Implementar otimizaÃ§Ã£o automÃ¡tica do despacho
3. Investir em infraestrutura de transmissÃ£o crÃ­tica

### Longo Prazo (1-3 anos)
1. Integrar dados meteorolÃ³gicos para prediÃ§Ã£o
2. Implementar machine learning para otimizaÃ§Ã£o
3. Desenvolver estratÃ©gias de armazenamento de energia

## ğŸ“ Suporte

Para dÃºvidas ou sugestÃµes sobre o projeto:

1. Verifique a documentaÃ§Ã£o dos arquivos
2. Execute os testes de exemplo
3. Consulte os logs de execuÃ§Ã£o
4. Entre em contato com a equipe de desenvolvimento

---

**Desenvolvido para anÃ¡lise de big-data em energia eÃ³lica** ğŸŒªï¸âš¡ 